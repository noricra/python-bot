"""
Scraper Twitter/X pour extraire les profils de cr√©ateurs et leurs emails
"""

import time
import csv
import re
from typing import List, Dict, Optional
from playwright.sync_api import sync_playwright, Page, Browser
from link_parser import BioLinkParser
from config import (
    DELAY_BETWEEN_REQUESTS,
    MIN_FOLLOWERS,
    USER_AGENT,
    OUTPUT_FILE
)


class TwitterScraper:
    """Scraper pour Twitter/X"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.link_parser = BioLinkParser()
        self.scraped_profiles = set()

    def start_browser(self):
        """D√©marre le navigateur Playwright"""
        print("üöÄ D√©marrage du navigateur...")
        playwright = sync_playwright().start()
        self.browser = playwright.chromium.launch(headless=self.headless)
        context = self.browser.new_context(
            user_agent=USER_AGENT,
            viewport={'width': 1920, 'height': 1080}
        )
        self.page = context.new_page()
        print("‚úÖ Navigateur pr√™t")

    def close_browser(self):
        """Ferme le navigateur"""
        if self.browser:
            self.browser.close()
            print("üî¥ Navigateur ferm√©")

    def parse_followers_count(self, text: str) -> int:
        """
        Parse le nombre de followers depuis le texte Twitter
        Ex: "1.2M" -> 1200000, "500K" -> 500000, "1,234" -> 1234
        """
        if not text:
            return 0

        text = text.strip().upper().replace(',', '').replace(' ', '')

        # Extraire les chiffres et la lettre (M/K)
        match = re.search(r'([\d.]+)([MK]?)', text)
        if not match:
            return 0

        number = float(match.group(1))
        suffix = match.group(2)

        if suffix == 'M':
            return int(number * 1_000_000)
        elif suffix == 'K':
            return int(number * 1_000)
        else:
            return int(number)

    def extract_profile_data(self, username: str) -> Optional[Dict]:
        """
        Visite un profil Twitter et extrait les donn√©es
        Retourne: {username, bio, email, links, followers}
        """
        try:
            profile_url = f"https://twitter.com/{username}"
            print(f"   üê¶ Visite du profil: @{username}")

            self.page.goto(profile_url, wait_until='networkidle', timeout=30000)
            time.sleep(DELAY_BETWEEN_REQUESTS)

            # Extraire la bio
            bio = ""
            try:
                # Twitter utilise plusieurs s√©lecteurs possibles pour la bio
                bio_selectors = [
                    '[data-testid="UserDescription"]',
                    '[data-testid="UserProfileHeader_Items"] span'
                ]

                for selector in bio_selectors:
                    bio_element = self.page.query_selector(selector)
                    if bio_element:
                        bio = bio_element.inner_text()
                        if bio:
                            break
            except:
                pass

            # Extraire le nombre de followers
            followers = 0
            try:
                # Chercher le lien "followers"
                followers_link = self.page.query_selector('a[href*="/followers"]')
                if followers_link:
                    followers_text = followers_link.inner_text()
                    # Le texte est genre "1.2K Followers"
                    followers = self.parse_followers_count(followers_text)
            except:
                pass

            # Filtrer si pas assez de followers
            if followers < MIN_FOLLOWERS:
                print(f"   ‚è≠Ô∏è  Ignor√© ({followers} followers < {MIN_FOLLOWERS})")
                return None

            # Extraire email direct de la bio
            email = self.link_parser.extract_email_from_text(bio)

            # Extraire les liens externes de la bio
            bio_links = []
            try:
                # Les liens dans la bio Twitter
                link_elements = self.page.query_selector_all('[data-testid="UserUrl"] a, [data-testid="UserDescription"] a')
                for link_elem in link_elements:
                    href = link_elem.get_attribute('href')
                    if href and href.startswith('http'):
                        bio_links.append(href)
            except:
                pass

            # Parser aussi les liens trouv√©s dans le texte de la bio
            text_links = self.link_parser.extract_bio_links(bio)
            bio_links.extend(text_links)
            bio_links = list(set(bio_links))  # Enlever doublons

            # Si pas d'email direct, parser les liens bio
            if not email and bio_links:
                print(f"   üîç Parsing des liens bio...")
                for link in bio_links[:2]:  # Limiter √† 2 liens
                    try:
                        email = self.link_parser.parse_bio_link(link)
                        if email:
                            print(f"   ‚úÖ Email trouv√©: {email}")
                            break
                    except Exception as e:
                        print(f"   ‚ùå Erreur parsing {link}: {e}")
                        continue

            profile_data = {
                'username': username,
                'profile_url': profile_url,
                'bio': bio,
                'email': email if email else "",
                'bio_links': ', '.join(bio_links),
                'followers': followers
            }

            if email:
                print(f"   ‚úÖ Profil scraped: @{username} ({followers} followers) - Email: {email}")
            else:
                print(f"   ‚ö†Ô∏è  Profil scraped: @{username} ({followers} followers) - Pas d'email")

            return profile_data

        except Exception as e:
            print(f"   ‚ùå Erreur extraction profil @{username}: {e}")
            return None

    def search_creators(self, keyword: str, limit: int = 20) -> List[str]:
        """
        Recherche des cr√©ateurs par mot-cl√© et retourne leurs usernames
        """
        print(f"\nüîé Recherche Twitter: '{keyword}'")

        try:
            # URL de recherche Twitter (people search)
            search_url = f"https://twitter.com/search?q={keyword.replace(' ', '%20')}&f=user"
            self.page.goto(search_url, wait_until='networkidle', timeout=30000)
            time.sleep(3)

            usernames = []

            # Scroller pour charger plus de r√©sultats
            for _ in range(3):
                self.page.evaluate("window.scrollBy(0, 1000)")
                time.sleep(2)

            # Extraire les usernames depuis les profils affich√©s
            # Twitter structure: <a href="/username">
            profile_links = self.page.query_selector_all('a[href^="/"][href*="/"]')

            for link in profile_links:
                try:
                    href = link.get_attribute('href')
                    if href:
                        # Format: /username ou /username/status/...
                        match = re.match(r'^/([a-zA-Z0-9_]+)$', href)
                        if match:
                            username = match.group(1)
                            # Filtrer les pages syst√®me de Twitter
                            if username not in ['home', 'explore', 'notifications', 'messages', 'search', 'settings', 'i']:
                                if username not in self.scraped_profiles:
                                    usernames.append(username)
                                    self.scraped_profiles.add(username)

                                    if len(usernames) >= limit:
                                        break
                except:
                    continue

            print(f"   ‚úÖ {len(usernames)} profils trouv√©s")
            return usernames[:limit]

        except Exception as e:
            print(f"   ‚ùå Erreur recherche '{keyword}': {e}")
            return []

    def scrape_all(self, keywords: List[str]) -> List[Dict]:
        """
        Scrape tous les profils pour une liste de mots-cl√©s
        """
        all_profiles = []

        try:
            self.start_browser()

            for keyword in keywords:
                # Rechercher des profils
                usernames = self.search_creators(keyword)

                # Extraire les donn√©es de chaque profil
                for username in usernames:
                    profile_data = self.extract_profile_data(username)
                    if profile_data:
                        all_profiles.append(profile_data)

                time.sleep(DELAY_BETWEEN_REQUESTS)

            print(f"\n‚úÖ Total scraped: {len(all_profiles)} profils")
            return all_profiles

        finally:
            self.close_browser()

    def save_to_csv(self, profiles: List[Dict], filename: str = "output/twitter_leads.csv"):
        """
        Sauvegarde les profils dans un fichier CSV
        """
        if not profiles:
            print("‚ö†Ô∏è  Aucun profil √† sauvegarder")
            return

        print(f"\nüíæ Sauvegarde dans {filename}...")

        fieldnames = ['username', 'profile_url', 'bio', 'email', 'bio_links', 'followers']

        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(profiles)

        # Stats
        total = len(profiles)
        with_email = len([p for p in profiles if p.get('email')])

        print(f"‚úÖ {total} profils sauvegard√©s")
        print(f"üìß {with_email} profils avec email ({with_email/total*100:.1f}%)")
        print(f"üìÇ Fichier: {filename}")


# Mots-cl√©s sp√©cifiques Twitter
TWITTER_KEYWORDS = [
    "digital products creator",
    "indie hacker",
    "solopreneur",
    "course creator",
    "ebook author",
    "template designer",
    "notion templates",
    "crypto creator",
    "web3 builder",
    "content monetization",
    "building in public",
    "bootstrapped founder",
    "micro saas",
    "info products"
]


def main():
    """
    Fonction principale
    """
    print("=" * 60)
    print("üê¶ TWITTER SCRAPER - Extraction d'emails de cr√©ateurs")
    print("=" * 60)

    scraper = TwitterScraper(headless=True)  # headless=False pour voir le navigateur

    # Scraper avec les mots-cl√©s
    profiles = scraper.scrape_all(TWITTER_KEYWORDS)

    # Sauvegarder les r√©sultats
    scraper.save_to_csv(profiles)

    print("\n‚úÖ TERMIN√â!")


if __name__ == "__main__":
    main()
